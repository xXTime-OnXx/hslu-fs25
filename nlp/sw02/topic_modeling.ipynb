{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53d81604-025d-4fe1-a130-6a978f5ba135",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "In this exercise, we will do topic modeling with gensim. Use the [topics and transformations tutorial](https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html) as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e45876ae-0f77-4bf8-8da4-b18618005327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e6efd1",
   "metadata": {},
   "source": [
    "For tokenizing words and stopword removal, download the NLTK punkt tokenizer and stopwords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edf524f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/timon/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /Users/timon/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee84f40-20bf-47da-b0b4-a0ff28f9b5cd",
   "metadata": {},
   "source": [
    "First, we load the [Lee Background Corpus](https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf) included with gensim that contains 300 news articles of the Australian Broadcasting Corporation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24d72e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "train_file = datapath('lee_background.cor')\n",
    "articles_orig = open(train_file).read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b2e56f",
   "metadata": {},
   "source": [
    "Preprocess the text by lowercasing, removing stopwords, stemming, and removing rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a870af-9f6b-43ea-940f-558e9a21bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stopword list\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords = stopwords | {'\\\"', '\\'', '\\'\\'', '`', '``', '\\'s'}\n",
    "\n",
    "# initialize stemmer\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def preprocess(article):\n",
    "    # tokenize\n",
    "    article = nltk.word_tokenize(article)\n",
    "\n",
    "    # lowercase all words\n",
    "    article = [word.lower() for word in article]\n",
    "\n",
    "    # remove stopwords\n",
    "    article = [word for word in article if word not in stopwords]\n",
    "\n",
    "    # optional: stem\n",
    "    # article = [stemmer.stem(word) for word in article]\n",
    "    return article\n",
    "\n",
    "articles = [preprocess(article) for article in articles_orig]\n",
    "\n",
    "# create the dictionary and corpus objects that gensim uses for topic modeling\n",
    "dictionary = gensim.corpora.Dictionary(articles)\n",
    "\n",
    "# remove words that occur in less than 2 documents, or more than 50% of documents\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.5)\n",
    "temp = dictionary[0]  # load the dictionary by calling it once\n",
    "corpus_bow = [dictionary.doc2bow(article) for article in articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5ae61a",
   "metadata": {},
   "source": [
    "\n",
    "Now we create a TF-IDF model and transform the corpus into TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fab13db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 7), (42, 1), (43, 1), (44, 1), (45, 3), (46, 1), (47, 1), (48, 2), (49, 2), (50, 3), (51, 3), (52, 1), (53, 2), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 2), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 8), (73, 1), (74, 1), (75, 1), (76, 2), (77, 1), (78, 1), (79, 2), (80, 1), (81, 1), (82, 3), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 5), (90, 1), (91, 2), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 3), (99, 1), (100, 1), (101, 3), (102, 1), (103, 1), (104, 1), (105, 4), (106, 2), (107, 1), (108, 1), (109, 1), (110, 1)]\n",
      "[(0, 0.045163832296308125), (1, 0.049004990699027966), (2, 0.09398031720792203), (3, 0.06797874731615453), (4, 0.08637534553463992), (5, 0.10158528888120417), (6, 0.058872481173046734), (7, 0.045871696227162966), (8, 0.04660732651093343), (9, 0.03476708703034139), (10, 0.09174339245432593), (11, 0.06379342938648586), (12, 0.08097953226203827), (13, 0.08637534553463992), (14, 0.06576958891547403), (15, 0.05748249959948285), (16, 0.07679421433236962), (17, 0.09398031720792203), (18, 0.04197717742438698), (19, 0.06379342938648586), (20, 0.09398031720792203), (21, 0.07679421433236962), (22, 0.08097953226203827), (23, 0.058872481173046734), (24, 0.05497796237027076), (25, 0.05497796237027076), (26, 0.07337456058875615), (27, 0.05497796237027076), (28, 0.08637534553463992), (29, 0.058872481173046734), (30, 0.062005775644911734), (31, 0.08637534553463992), (32, 0.09398031720792203), (33, 0.04737299069698862), (34, 0.07048328454536662), (35, 0.09398031720792203), (36, 0.09398031720792203), (37, 0.07679421433236962), (38, 0.06379342938648586), (39, 0.09398031720792203), (40, 0.05276880396959025), (41, 0.3161468260741569), (42, 0.06576958891547403), (43, 0.06576958891547403), (44, 0.04197717742438698), (45, 0.1860173269347352), (46, 0.08637534553463992), (47, 0.09398031720792203), (48, 0.17275069106927984), (49, 0.15358842866473923), (50, 0.1973087667464221), (51, 0.19138028815945754), (52, 0.06379342938648586), (53, 0.18796063441584407), (54, 0.07679421433236962), (55, 0.05384087678041912), (56, 0.07679421433236962), (57, 0.07679421433236962), (58, 0.08637534553463992), (59, 0.04318767276731996), (60, 0.13595749463230905), (61, 0.07048328454536662), (62, 0.06797874731615453), (63, 0.04318767276731996), (64, 0.08637534553463992), (65, 0.04448171465359908), (66, 0.049877527926200725), (67, 0.07337456058875615), (68, 0.05175471008582299), (69, 0.029876861457627475), (70, 0.043823535964961836), (71, 0.07337456058875615), (72, 0.1663540992526395), (73, 0.048171245973727274), (74, 0.09398031720792203), (75, 0.062005775644911734), (76, 0.04274284161044218), (77, 0.07337456058875615), (78, 0.06037377564287238), (79, 0.18796063441584407), (80, 0.09398031720792203), (81, 0.06379342938648586), (82, 0.23038264299710884), (83, 0.05618845771320373), (84, 0.08097953226203827), (85, 0.06379342938648586), (86, 0.07048328454536662), (87, 0.05384087678041912), (88, 0.06797874731615453), (89, 0.14342796675805272), (90, 0.07679421433236962), (91, 0.10995592474054151), (92, 0.06379342938648586), (93, 0.03976801902370649), (94, 0.0360042057531442), (95, 0.06797874731615453), (96, 0.07679421433236962), (97, 0.058872481173046734), (98, 0.11930405707111948), (99, 0.07679421433236962), (100, 0.030502124955654616), (101, 0.1860173269347352), (102, 0.05618845771320373), (103, 0.058872481173046734), (104, 0.08097953226203827), (105, 0.17529414385984735), (106, 0.11237691542640746), (107, 0.045871696227162966), (108, 0.08097953226203827), (109, 0.06037377564287238), (110, 0.03398546693692743)]\n"
     ]
    }
   ],
   "source": [
    "model_tfidf = models.TfidfModel(corpus_bow)\n",
    "\n",
    "corpus_tfidf = model_tfidf[corpus_bow]\n",
    "print(corpus_bow[0])\n",
    "print(corpus_tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24df8cb",
   "metadata": {},
   "source": [
    "Now we train an [LDA model](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html) with 10 topics on the TF-IDF corpus. Save it to a variable `model_lda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ded6b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lda = models.LdaModel(\n",
    "    corpus=corpus_bow,\n",
    "    id2word=dictionary,\n",
    "    num_topics=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91845654",
   "metadata": {},
   "source": [
    "Let's inspect the first 5 topics of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca3a357e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3,\n",
       "  '0.012*\"australia\" + 0.009*\"mr\" + 0.007*\"australian\" + 0.005*\"two\" + 0.005*\"year\" + 0.005*\"world\" + 0.005*\"india\" + 0.005*\"also\" + 0.005*\"police\" + 0.004*\"last\"'),\n",
       " (7,\n",
       "  '0.015*\"mr\" + 0.007*\"us\" + 0.005*\"afghanistan\" + 0.005*\"people\" + 0.005*\"bin\" + 0.005*\"states\" + 0.005*\"united\" + 0.004*\"two\" + 0.004*\"australian\" + 0.004*\"laden\"'),\n",
       " (9,\n",
       "  '0.008*\"government\" + 0.007*\"new\" + 0.007*\"mr\" + 0.007*\"us\" + 0.006*\"afghanistan\" + 0.006*\"people\" + 0.005*\"south\" + 0.005*\"qantas\" + 0.004*\"australia\" + 0.004*\"two\"'),\n",
       " (5,\n",
       "  '0.007*\"mr\" + 0.007*\"palestinian\" + 0.006*\"new\" + 0.006*\"people\" + 0.005*\"australian\" + 0.005*\"would\" + 0.004*\"fire\" + 0.004*\"police\" + 0.004*\"government\" + 0.004*\"last\"'),\n",
       " (2,\n",
       "  '0.011*\"mr\" + 0.007*\"palestinian\" + 0.005*\"arafat\" + 0.005*\"israeli\" + 0.005*\"australia\" + 0.004*\"minister\" + 0.004*\"test\" + 0.004*\"new\" + 0.004*\"security\" + 0.004*\"first\"')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lda.print_topics(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ce453",
   "metadata": {},
   "source": [
    "We see the 5 topics with the highest importance. For each topic, the 10 most important words are shown, together with their coefficient of \"alignment\" to the topic.\n",
    "\n",
    "## Document Similarity\n",
    "We now use our LDA model to compare the similarity of new documents (*queries*) to documents in our collection.\n",
    "\n",
    "First, create an index of the news articles in our corpus. Use the `MatrixSimilarity` transformation as described in gensim's [similarity queries tutorial](https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4eb44cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "\n",
    "index = similarities.MatrixSimilarity(model_lda[corpus_tfidf])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7b2c1f",
   "metadata": {},
   "source": [
    "Now, write a function that takes a query string as input and returns the LDA representation for it. Make sure to apply the same preprocessing as we did to the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabf9dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"Human computer interaction\"\n",
    "# TODO: preprocessing before doc2bow conversion\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow]  # convert the query to LSI space\n",
    "print(vec_lsi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77753be",
   "metadata": {},
   "source": [
    "Print the top 5 most similar documents, together with their similarities, using your index created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7696f2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56e05dba",
   "metadata": {},
   "source": [
    "Run your code again, now training an LDA model with 100 topics. Do you see a qualitative difference in the top-5 most similar documents?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
