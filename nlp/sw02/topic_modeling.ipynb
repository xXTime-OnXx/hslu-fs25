{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53d81604-025d-4fe1-a130-6a978f5ba135",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "In this exercise, we will do topic modeling with gensim. Use the [topics and transformations tutorial](https://radimrehurek.com/gensim/auto_examples/core/run_topics_and_transformations.html) as a reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e45876ae-0f77-4bf8-8da4-b18618005327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import gensim\n",
    "import nltk\n",
    "\n",
    "from gensim import models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e6efd1",
   "metadata": {},
   "source": [
    "For tokenizing words and stopword removal, download the NLTK punkt tokenizer and stopwords list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edf524f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/timon/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/timon/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee84f40-20bf-47da-b0b4-a0ff28f9b5cd",
   "metadata": {},
   "source": [
    "First, we load the [Lee Background Corpus](https://hekyll.services.adelaide.edu.au/dspace/bitstream/2440/28910/1/hdl_28910.pdf) included with gensim that contains 300 news articles of the Australian Broadcasting Corporation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24d72e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "train_file = datapath('lee_background.cor')\n",
    "articles_orig = open(train_file).read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b2e56f",
   "metadata": {},
   "source": [
    "Preprocess the text by lowercasing, removing stopwords, stemming, and removing rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88a870af-9f6b-43ea-940f-558e9a21bb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stopword list\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stopwords = stopwords | {'\\\"', '\\'', '\\'\\'', '`', '``', '\\'s'}\n",
    "\n",
    "# initialize stemmer\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "\n",
    "def preprocess(article):\n",
    "    # tokenize\n",
    "    article = nltk.word_tokenize(article)\n",
    "\n",
    "    # lowercase all words\n",
    "    article = [word.lower() for word in article]\n",
    "\n",
    "    # remove stopwords\n",
    "    article = [word for word in article if word not in stopwords]\n",
    "\n",
    "    # optional: stem\n",
    "    # article = [stemmer.stem(word) for word in article]\n",
    "    return article\n",
    "\n",
    "articles = [preprocess(article) for article in articles_orig]\n",
    "\n",
    "# create the dictionary and corpus objects that gensim uses for topic modeling\n",
    "dictionary = gensim.corpora.Dictionary(articles)\n",
    "\n",
    "# remove words that occur in less than 2 documents, or more than 50% of documents\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.5)\n",
    "temp = dictionary[0]  # load the dictionary by calling it once\n",
    "corpus_bow = [dictionary.doc2bow(article) for article in articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5ae61a",
   "metadata": {},
   "source": [
    "\n",
    "Now we create a TF-IDF model and transform the corpus into TF-IDF vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fab13db5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2), (6, 1), (7, 1), (8, 1), (9, 1), (10, 2), (11, 1), (12, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 7), (42, 1), (43, 1), (44, 1), (45, 3), (46, 1), (47, 1), (48, 2), (49, 2), (50, 3), (51, 3), (52, 1), (53, 2), (54, 1), (55, 1), (56, 1), (57, 1), (58, 1), (59, 1), (60, 2), (61, 1), (62, 1), (63, 1), (64, 1), (65, 1), (66, 1), (67, 1), (68, 1), (69, 1), (70, 1), (71, 1), (72, 8), (73, 1), (74, 1), (75, 1), (76, 2), (77, 1), (78, 1), (79, 2), (80, 1), (81, 1), (82, 3), (83, 1), (84, 1), (85, 1), (86, 1), (87, 1), (88, 1), (89, 5), (90, 1), (91, 2), (92, 1), (93, 1), (94, 1), (95, 1), (96, 1), (97, 1), (98, 3), (99, 1), (100, 1), (101, 3), (102, 1), (103, 1), (104, 1), (105, 4), (106, 2), (107, 1), (108, 1), (109, 1), (110, 1)]\n",
      "[(0, 0.045163832296308125), (1, 0.049004990699027966), (2, 0.09398031720792203), (3, 0.06797874731615453), (4, 0.08637534553463992), (5, 0.10158528888120417), (6, 0.058872481173046734), (7, 0.045871696227162966), (8, 0.04660732651093343), (9, 0.03476708703034139), (10, 0.09174339245432593), (11, 0.06379342938648586), (12, 0.08097953226203827), (13, 0.08637534553463992), (14, 0.06576958891547403), (15, 0.05748249959948285), (16, 0.07679421433236962), (17, 0.09398031720792203), (18, 0.04197717742438698), (19, 0.06379342938648586), (20, 0.09398031720792203), (21, 0.07679421433236962), (22, 0.08097953226203827), (23, 0.058872481173046734), (24, 0.05497796237027076), (25, 0.05497796237027076), (26, 0.07337456058875615), (27, 0.05497796237027076), (28, 0.08637534553463992), (29, 0.058872481173046734), (30, 0.062005775644911734), (31, 0.08637534553463992), (32, 0.09398031720792203), (33, 0.04737299069698862), (34, 0.07048328454536662), (35, 0.09398031720792203), (36, 0.09398031720792203), (37, 0.07679421433236962), (38, 0.06379342938648586), (39, 0.09398031720792203), (40, 0.05276880396959025), (41, 0.3161468260741569), (42, 0.06576958891547403), (43, 0.06576958891547403), (44, 0.04197717742438698), (45, 0.1860173269347352), (46, 0.08637534553463992), (47, 0.09398031720792203), (48, 0.17275069106927984), (49, 0.15358842866473923), (50, 0.1973087667464221), (51, 0.19138028815945754), (52, 0.06379342938648586), (53, 0.18796063441584407), (54, 0.07679421433236962), (55, 0.05384087678041912), (56, 0.07679421433236962), (57, 0.07679421433236962), (58, 0.08637534553463992), (59, 0.04318767276731996), (60, 0.13595749463230905), (61, 0.07048328454536662), (62, 0.06797874731615453), (63, 0.04318767276731996), (64, 0.08637534553463992), (65, 0.04448171465359908), (66, 0.049877527926200725), (67, 0.07337456058875615), (68, 0.05175471008582299), (69, 0.029876861457627475), (70, 0.043823535964961836), (71, 0.07337456058875615), (72, 0.1663540992526395), (73, 0.048171245973727274), (74, 0.09398031720792203), (75, 0.062005775644911734), (76, 0.04274284161044218), (77, 0.07337456058875615), (78, 0.06037377564287238), (79, 0.18796063441584407), (80, 0.09398031720792203), (81, 0.06379342938648586), (82, 0.23038264299710884), (83, 0.05618845771320373), (84, 0.08097953226203827), (85, 0.06379342938648586), (86, 0.07048328454536662), (87, 0.05384087678041912), (88, 0.06797874731615453), (89, 0.14342796675805272), (90, 0.07679421433236962), (91, 0.10995592474054151), (92, 0.06379342938648586), (93, 0.03976801902370649), (94, 0.0360042057531442), (95, 0.06797874731615453), (96, 0.07679421433236962), (97, 0.058872481173046734), (98, 0.11930405707111948), (99, 0.07679421433236962), (100, 0.030502124955654616), (101, 0.1860173269347352), (102, 0.05618845771320373), (103, 0.058872481173046734), (104, 0.08097953226203827), (105, 0.17529414385984735), (106, 0.11237691542640746), (107, 0.045871696227162966), (108, 0.08097953226203827), (109, 0.06037377564287238), (110, 0.03398546693692743)]\n"
     ]
    }
   ],
   "source": [
    "model_tfidf = models.TfidfModel(corpus_bow)\n",
    "\n",
    "corpus_tfidf = model_tfidf[corpus_bow]\n",
    "print(corpus_bow[0])\n",
    "print(corpus_tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24df8cb",
   "metadata": {},
   "source": [
    "Now we train an [LDA model](https://radimrehurek.com/gensim/auto_examples/tutorials/run_lda.html) with 10 topics on the TF-IDF corpus. Save it to a variable `model_lda`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ded6b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lda = models.LdaModel(\n",
    "    corpus=corpus_bow,\n",
    "    id2word=dictionary,\n",
    "    num_topics=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91845654",
   "metadata": {},
   "source": [
    "Let's inspect the first 5 topics of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca3a357e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9,\n",
       "  '0.008*\"palestinian\" + 0.007*\"mr\" + 0.006*\"south\" + 0.005*\"us\" + 0.005*\"bin\" + 0.005*\"new\" + 0.005*\"laden\" + 0.004*\"taliban\" + 0.004*\"one\" + 0.004*\"also\"'),\n",
       " (8,\n",
       "  '0.007*\"mr\" + 0.005*\"last\" + 0.005*\"us\" + 0.005*\"people\" + 0.005*\"one\" + 0.004*\"south\" + 0.004*\"australia\" + 0.004*\"two\" + 0.004*\"new\" + 0.004*\"would\"'),\n",
       " (1,\n",
       "  '0.009*\"mr\" + 0.007*\"new\" + 0.007*\"australia\" + 0.007*\"people\" + 0.006*\"australian\" + 0.005*\"government\" + 0.004*\"would\" + 0.004*\"say\" + 0.004*\"us\" + 0.004*\"per\"'),\n",
       " (0,\n",
       "  '0.009*\"mr\" + 0.007*\"people\" + 0.006*\"australian\" + 0.005*\"palestinian\" + 0.005*\"australia\" + 0.005*\"new\" + 0.005*\"us\" + 0.005*\"also\" + 0.005*\"two\" + 0.005*\"police\"'),\n",
       " (5,\n",
       "  '0.011*\"mr\" + 0.008*\"government\" + 0.008*\"australian\" + 0.006*\"afghanistan\" + 0.005*\"south\" + 0.005*\"first\" + 0.005*\"new\" + 0.005*\"australia\" + 0.005*\"security\" + 0.005*\"palestinian\"')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lda.print_topics(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ce453",
   "metadata": {},
   "source": [
    "We see the 5 topics with the highest importance. For each topic, the 10 most important words are shown, together with their coefficient of \"alignment\" to the topic.\n",
    "\n",
    "## Document Similarity\n",
    "We now use our LDA model to compare the similarity of new documents (*queries*) to documents in our collection.\n",
    "\n",
    "First, create an index of the news articles in our corpus. Use the `MatrixSimilarity` transformation as described in gensim's [similarity queries tutorial](https://radimrehurek.com/gensim/auto_examples/core/run_similarity_queries.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4eb44cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "\n",
    "index = similarities.MatrixSimilarity(model_lda[corpus_tfidf])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7b2c1f",
   "metadata": {},
   "source": [
    "Now, write a function that takes a query string as input and returns the LDA representation for it. Make sure to apply the same preprocessing as we did to the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dabf9dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.05000383), (1, 0.050005518), (2, 0.050006784), (3, 0.050003476), (4, 0.050005294), (5, 0.050003637), (6, 0.050004765), (7, 0.05000939), (8, 0.54995304), (9, 0.05000425)]\n"
     ]
    }
   ],
   "source": [
    "query = \"Human computer interaction\"\n",
    "pre_query = preprocess(query)\n",
    "\n",
    "vec_bow = dictionary.doc2bow(pre_query)\n",
    "vec_lsi = model_lda[vec_bow]  # convert the query to LSI space\n",
    "print(vec_lsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77753be",
   "metadata": {},
   "source": [
    "Print the top 5 most similar documents, together with their similarities, using your index created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7696f2f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.12362331), (1, 0.10999815), (2, 0.11755372), (3, 0.10955433), (4, 0.11260693), (5, 0.108977884), (6, 0.68706435), (7, 0.11419714), (8, 0.14642076), (9, 0.12079371), (10, 0.11103848), (11, 0.112090275), (12, 0.10658221), (13, 0.15727511), (14, 0.114781186), (15, 0.11308652), (16, 0.1168414), (17, 0.15305422), (18, 0.1179017), (19, 0.11460081), (20, 0.11931464), (21, 0.979245), (22, 0.1176006), (23, 0.11233145), (24, 0.115853705), (25, 0.14794421), (26, 0.97615093), (27, 0.11531561), (28, 0.97474885), (29, 0.110181294), (30, 0.115607515), (31, 0.11005247), (32, 0.12687908), (33, 0.0877205), (34, 0.11033183), (35, 0.11082268), (36, 0.12687851), (37, 0.0877205), (38, 0.97555023), (39, 0.110385455), (40, 0.0877196), (41, 0.11470556), (42, 0.110219136), (43, 0.975523), (44, 0.10901283), (45, 0.11098352), (46, 0.14400078), (47, 0.11189617), (48, 0.10598667), (49, 0.13254867), (50, 0.112762526), (51, 0.11673851), (52, 0.10686405), (53, 0.11523597), (54, 0.97629166), (55, 0.10875548), (56, 0.11424477), (57, 0.11361709), (58, 0.120298736), (59, 0.11006066), (60, 0.106595665), (61, 0.97444427), (62, 0.11137783), (63, 0.11463403), (64, 0.10915411), (65, 0.112169735), (66, 0.10952251), (67, 0.1630301), (68, 0.117199704), (69, 0.11006048), (70, 0.114154086), (71, 0.12430724), (72, 0.11599663), (73, 0.97684956), (74, 0.08771852), (75, 0.11106195), (76, 0.10455843), (77, 0.11036879), (78, 0.9740061), (79, 0.106127776), (80, 0.11469227), (81, 0.12196056), (82, 0.08771852), (83, 0.9758317), (84, 0.087728605), (85, 0.11526183), (86, 0.11564653), (87, 0.12403696), (88, 0.08771852), (89, 0.10963067), (90, 0.10853193), (91, 0.11528435), (92, 0.9754836), (93, 0.08772182), (94, 0.112084575), (95, 0.11530757), (96, 0.10614277), (97, 0.11328276), (98, 0.08771852), (99, 0.10964554), (100, 0.11586321), (101, 0.111186), (102, 0.11288499), (103, 0.116612524), (104, 0.08771852), (105, 0.087728605), (106, 0.0877196), (107, 0.08771852), (108, 0.72816867), (109, 0.10901374), (110, 0.11389828), (111, 0.1168122), (112, 0.08771852), (113, 0.111712195), (114, 0.89118946), (115, 0.10974453), (116, 0.08772404), (117, 0.9751053), (118, 0.11476995), (119, 0.10974441), (120, 0.97510535), (121, 0.11436443), (122, 0.15339269), (123, 0.1165584), (124, 0.97821945), (125, 0.111125745), (126, 0.11178519), (127, 0.10775892), (128, 0.111580744), (129, 0.10715506), (130, 0.15003407), (131, 0.122183844), (132, 0.10689985), (133, 0.10700239), (134, 0.10721678), (135, 0.11270576), (136, 0.11519835), (137, 0.11556567), (138, 0.08771852), (139, 0.15171938), (140, 0.107182294), (141, 0.11099201), (142, 0.112727895), (143, 0.116250135), (144, 0.9741609), (145, 0.0877196), (146, 0.11202061), (147, 0.08771823), (148, 0.111946926), (149, 0.11527654), (150, 0.106130175), (151, 0.9647511), (152, 0.08772404), (153, 0.12385544), (154, 0.11291225), (155, 0.977937), (156, 0.10613012), (157, 0.09694064), (158, 0.9775174), (159, 0.11002728), (160, 0.10953782), (161, 0.60135025), (162, 0.118043706), (163, 0.11184074), (164, 0.11676476), (165, 0.11066719), (166, 0.11221226), (167, 0.08771886), (168, 0.14715609), (169, 0.11452095), (170, 0.14728458), (171, 0.12084962), (172, 0.108723655), (173, 0.11294074), (174, 0.120435424), (175, 0.1128631), (176, 0.10647716), (177, 0.11062892), (178, 0.11127376), (179, 0.1474861), (180, 0.115553446), (181, 0.110490605), (182, 0.10800215), (183, 0.10994862), (184, 0.123136446), (185, 0.107059516), (186, 0.97667605), (187, 0.11278448), (188, 0.11090415), (189, 0.122769624), (190, 0.11418083), (191, 0.08772142), (192, 0.10647865), (193, 0.115964554), (194, 0.97503656), (195, 0.111219026), (196, 0.11995024), (197, 0.10991175), (198, 0.11419591), (199, 0.11329702), (200, 0.0877196), (201, 0.106385134), (202, 0.112255715), (203, 0.13323517), (204, 0.10897204), (205, 0.11655059), (206, 0.11153346), (207, 0.12423448), (208, 0.11224159), (209, 0.116354994), (210, 0.14829466), (211, 0.1093074), (212, 0.109392576), (213, 0.10724178), (214, 0.106394574), (215, 0.11517449), (216, 0.115232736), (217, 0.15122496), (218, 0.11549328), (219, 0.11545261), (220, 0.10649102), (221, 0.11121161), (222, 0.122270815), (223, 0.11050573), (224, 0.10735444), (225, 0.120745815), (226, 0.11564166), (227, 0.11021772), (228, 0.14528935), (229, 0.15418684), (230, 0.10923905), (231, 0.11212067), (232, 0.106018774), (233, 0.119752705), (234, 0.11023958), (235, 0.11242828), (236, 0.109239034), (237, 0.1149755), (238, 0.114633456), (239, 0.11276327), (240, 0.113809735), (241, 0.0877205), (242, 0.114996314), (243, 0.0877196), (244, 0.10915856), (245, 0.10924514), (246, 0.15678817), (247, 0.9753814), (248, 0.1588746), (249, 0.08771852), (250, 0.08771823), (251, 0.10441787), (252, 0.111577176), (253, 0.11665515), (254, 0.10915419), (255, 0.108584404), (256, 0.11414583), (257, 0.11269065), (258, 0.11583677), (259, 0.09070557), (260, 0.106548935), (261, 0.11356009), (262, 0.111706965), (263, 0.11110135), (264, 0.10863958), (265, 0.15350507), (266, 0.16081135), (267, 0.12404698), (268, 0.118939), (269, 0.11454996), (270, 0.10879734), (271, 0.11110135), (272, 0.12293573), (273, 0.10828477), (274, 0.9738676), (275, 0.08771886), (276, 0.12080506), (277, 0.10987513), (278, 0.9769845), (279, 0.10926143), (280, 0.11946326), (281, 0.97434264), (282, 0.1125009), (283, 0.121563375), (284, 0.10738618), (285, 0.13931262), (286, 0.11500657), (287, 0.10823751), (288, 0.97434264), (289, 0.112878114), (290, 0.12622319), (291, 0.11002768), (292, 0.11279605), (293, 0.11126524), (294, 0.1151798), (295, 0.9758337), (296, 0.11414477), (297, 0.11631882), (298, 0.11698611), (299, 0.10747116)]\n",
      "0.979245 ['nation', 'road', 'toll', 'risen', '37', ',', 'another', 'death', 'new', 'south', 'wales', 'roads', '.', '38-year-old', 'man', ',', 'injured', 'crash', 'mid-north', 'coast', 'last', 'week', ',', 'died', 'hospital', '.', '16-year-old', 'boy', ',', 'passenger', 'car', 'hit', 'telephone', 'pole', ',', 'remains', 'critical', 'condition', '.', 'new', 'south', 'wales', 'recorded', '18', 'holiday', 'deaths', '.', 'seven', 'people', 'died', 'queensland', 'roads', ',', 'five', 'victoria', ',', 'three', 'northern', 'territory', ',', 'two', 'western', 'australia', 'south', 'australia', '.', 'act', 'tasmania', 'remain', 'fatality', 'free', '.']\n",
      "0.97821945 ['federal', 'opposition', 'wants', 'tougher', 'penalties', 'ships', 'spill', 'oil', 'last', 'week', 'spill', 'affected', 'phillip', 'island', '.', 'volunteers', 'hope', 'clean', 'last', 'oil', 'phillip', 'island', 'today', '.', 'authorities', 'still', 'trying', 'track', 'source', 'spill', ',', 'affected', '360', 'fairy', 'penguins', '.', 'shadow', 'environment', 'minister', 'kelvin', 'thomson', 'says', 'better', 'deterrents', 'needed', 'prevent', 'spills', '.', 'one', 'problems', 'oil', 'spills', 'appears', 'lack', 'prosecution', 'action', 'believe', 'important', 'people', 'recklessly', 'negligently', 'cause', 'oil', 'spills', ',', 'seriously', 'damage', 'environment', 'subject', 'prosecution', 'action', ',', 'said', '.']\n",
      "0.977937 ['former', 'managing', 'director', 'one.tel', 'denied', 'claims', 'mislead', 'board', 'company', 'effectively', 'insolvent', '.', 'jodee', 'rich', 'says', 'civil', 'proceedings', 'brought', 'australian', 'securities', 'investment', 'commission', '(', 'asic', ')', ',', 'two', 'former', 'directors', 'former', 'chairman', ',', 'baseless', '.', 'asic', 'claims', 'true', 'financial', 'position', 'one.tel', 'made', 'known', 'directors', 'company', '.', 'however', ',', 'mr', 'rich', 'says', 'discrepancies', 'way', 'asic', 'handled', 'investigation', '.', 'concerned', 'part', 'process', 'asic', 'given', 'us', 'opportunity', 'respond', 'many', 'claims', 'alleged', 'six', 'months', 'ago', \"n't\", 'opportunity', 'respond', 'claims', 'bringing', 'us', ',', 'said', '.']\n",
      "0.9775174 ['afl', 'leading', 'goal', 'kicker', ',', 'tony', 'lockett', ',', 'nominate', 'pre-season', 'draft', '.', 'lockett', 'approached', 'sydney', 'swans', 'return', 'game', 'last', 'week', 'much', 'media', 'speculation', ',', 'decided', 'best', 'interests', 'family', 'come', 'retirement', '.', 'today', ',', '35-year-old', 'changed', 'mind', '.', 'informed', 'swans', 'intention', 'nominate', 'next', 'tuesday', 'pre-season', 'draft', '.', 'statement', 'released', 'short', 'time', 'ago', ',', 'lockett', 'says', 'last', 'week', 'felt', 'rushed', 'feel', 'comfortable', 'decision', '.', 'says', 'weekend', 'time', 'think', 'matter', 'family', ',', 'support', 'comeback', '.', 'sydney', 'says', 'delighted', 'lockett', 'decided', 'make', 'return', 'intends', 'draft', '.']\n",
      "0.9769845 ['royal', 'commission', 'looking', 'collapse', 'insurance', 'giant', 'hih', 'says', 'possible', 'leak', 'confidential', 'document', 'criminal', 'offence', '.', 'royal', 'commissioner', 'justice', 'neville', 'owen', 'opened', 'public', 'hearings', 'collapse', ',', 'eight', 'months', 'company', 'placed', 'provisional', 'liquidation', '.', 'opening', 'statement', ',', 'justice', 'owen', 'called', 'parties', 'adhere', 'confidentiality', 'requirements', 'royal', 'commission', '.', 'justice', 'owen', 'says', 'could', 'leak', 'report', 'role', 'auditors', ',', 'circulated', 'early', 'november', '.', 'possible', 'someone', ',', 'commission', 'delivered', 'copy', 'report', 'strict', 'confidence', 'disclosed', 'contents', 'author', 'article', '.', ',', 'may', 'breach', 'section', '6b', '(', '4', ')', 'royal', 'commissions', 'act', '1902', ',', 'criminal', 'offence', ',', 'said', '.']\n"
     ]
    }
   ],
   "source": [
    "sims = index[vec_lsi]  # perform a similarity query against the corpus\n",
    "print(list(enumerate(sims)))  # print (document_number, document_similarity) 2-tuples\n",
    "\n",
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "for doc_position, doc_score in sims[:5]:\n",
    "    print(doc_score, articles[doc_position])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e05dba",
   "metadata": {},
   "source": [
    "Run your code again, now training an LDA model with 100 topics. Do you see a qualitative difference in the top-5 most similar documents?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
